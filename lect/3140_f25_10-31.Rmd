---
title: "Phase II: Using Our Toolbox"
subtitle: "Module 6: Spatial Awareness"
author: "Dr. Christopher Kenaley"
institute: "Boston College"
date: "2025/10/31"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis"]
    lib_dir: libs
    nature:
      ratio: 16:9
---
class: top
# In class today 

```{r,echo=FALSE,message=FALSE,warning=F}
library(tidyverse)
library(kableExtra)
library(sf)
library(stars)
library(mapview)
library(randomForest)

knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 40), tidy = TRUE)


```

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css">


.pull-left[
Today we'll ....

- Intro to random forest models


]

.pull-right[


<img src="https://dfzljdn9uc3pi.cloudfront.net/2020/8262/1/fig-1-2x.jpg" width="450">
]

---

# What is a Random Forest?

.pull-left[
- **Ensemble method** built from many decision trees  
- Each tree trained on a **bootstrap sample** of the data  
- Predictions are averaged (regression) or voted (classification)  
- Reduces overfitting and improves generalization  
]

.pull-right[
![](https://miro.medium.com/v2/resize:fit:1184/format:webp/1*i0o8mjFfCn-uD79-F1Cqkw.png)
]

---

# 🌲 What is a Random Forest?

.pull-left[
### Think of it like a team of decision trees!

- Each **tree** makes a prediction (like a vote).  
- Trees are trained on **different random subsets** of the data.  
- Each tree also sees **different random sets of variables**.  
- The forest’s final prediction is the **average (regression)**  
  or **majority vote (classification)** across all trees.

💡 The idea:  
> Many weak but diverse trees together make a strong, stable model.
]

.pull-right[
![](https://miro.medium.com/v2/resize:fit:1184/format:webp/1*i0o8mjFfCn-uD79-F1Cqkw.png)

.small[Each tree sees a different sample of data and features.]
]


---

# 🌲 Classification vs. Regression Forests

.pull-left[
### **Classification Forest**
- Used when the **response variable is a category**  
  (e.g., species, gender, habitat type, house color).
- Each tree “votes” for a class label.
- The **majority vote** across all trees = prediction.

📘 **Example**
```{r}
Species ~ Sepal.Length + Petal.Width
#Predicts setosa, versicolor, or virginica.
```

🎯 Goal: minimize misclassification error.
]

.pull-right[

### **Regression Forest**
- Used when the response variable is numeric
(e.g., body mass, growth rate, temperature).
- Each tree predicts a number.
- The average across all trees = prediction.

📘 Example

```{r}
Sepal.Width ~ Sepal.Length + Petal.Width
```

🎯 Goal: minimize mean squared error (MSE).
]


---

# Installing and loading

```{r}
#install.packages("randomForest")
library(randomForest)

#Example dataset:
  
data(iris)
head(iris)
```


---

# ⚙️ Fitting a Random Forest in R
## 🔧 Steps
.pull-left[

### 1. Fit model
```{r}
set.seed(123) #for reproducibility
rf_model <- randomForest(
  Species ~ .,      # response ~ predictors
  data = iris,
  ntree = 500,      # number of trees
  mtry = 2, # predictors per split
  importance = TRUE # compute variable importance
)
```
]

.pull-right[

### What these do

- `ntree` → more trees = smoother, more stable forest
- `mtry` → how many predictors are tested at each split
- `importance` → tracks which variables matter most

]

---

# ⚙️ Fitting a Random Forest in R

.pull-left[
### 2. Check the model output

```{r}
rf_model

```
]

.pull-right[

### What these do
- check model output

]

---

# ⚙️ Fitting a Random Forest in R
.pull-left[
### 2. Check the model output

```{r,fig.height=4}
plot(rf_model)
```
]

.pull-right[

### What these do
- `plot()` → shows error rate vs. number of trees

]

---

# 🌟 What Variables are Important?

.pull-left[
### 🧠 The big idea

- A **Random Forest** learns from many predictors (features).  
- But not all predictors help equally!  
- **Variable importance** shows which ones matter most  
  for making good predictions.

💡 Imagine asking:  
> “If I hide this variable, does the forest get worse at predicting?”

If accuracy drops a lot → that variable is **important**.  
If accuracy stays the same → that variable probably isn’t helping much.
]

.pull-right[
### 🔍 Assessing Importances

```{r}
# Show how important each variable is
importance(rf_model)


```


---

# 🌟 What Variables are Important?

.pull-left[
### 🧠 The big idea

- A **Random Forest** learns from many predictors (features).  
- But not all predictors help equally!  
- **Variable importance** shows which ones matter most  
  for making good predictions.

💡 Imagine asking:  
> “If I hide this variable, does the forest get worse at predicting?”

If accuracy drops a lot → that variable is **important**.  
If accuracy stays the same → that variable probably isn’t helping much.
]

.pull-right[
### 🔍 Assessing Importances

```{r,fig.height=4}
# Make a quick plot
varImpPlot(rf_model)
```
.small[
  longer bars `=` variables that help the model the most.

shorter bars `=` variables with little impact on predictions.
]
]

---

# 🎯 Making Predictions with a Random Forest

.pull-left[
### 🧠 The idea

Once your forest is trained,  
you can use it to **predict new outcomes**!

- The model has “learned” patterns from training data.  
- You give it new observations (with predictor values).  
- Each tree makes a prediction →  
  the forest combines them for a final answer.

🌳 Many trees → one decision!
]

.pull-right[
### 🔍 In R

```{r}
# Make predictions
new_obs <- iris[1:5, -5]    # drop Species column
predict(rf_model, new_obs)
```
🧩 For classification:

Each tree votes for a class. The forest picks the majority vote.

📈 For regression:

Each tree predicts a number. The forest takes the average.


]

---

# 🧩 Comparing Predictions to Actual Classes

.pull-left[
### 🎯 Why compare?
- We want to see **how often the model gets it right!**  
- Comparing predicted and actual classes tells us  
  how accurate our Random Forest is.  
- A **confusion matrix** summarizes correct vs. incorrect predictions.

💡 The more predictions on the diagonal →  
the better the model!
]

.pull-right[

### Compare predictions to actual
```{r}
pred <- predict(rf_model, iris)
# Confusion matrix
table(Predicted = pred, Actual = iris$Species)
# Model accuracy
mean(pred == iris$Species)

```
.small[

The confusion matrix shows where the model is right or wrong.

Accuracy = proportion of correct predictions.

]
]