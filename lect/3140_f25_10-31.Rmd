---
title: "Phase II: Using Our Toolbox"
subtitle: "Module 6: Spatial Awareness"
author: "Dr. Christopher Kenaley"
institute: "Boston College"
date: "2025/10/31"
output:
  xaringan::moon_reader:
    css: ["default", "metropolis"]
    lib_dir: libs
    nature:
      ratio: 16:9
---
class: top
# In class today 

```{r,echo=FALSE,message=FALSE,warning=F}
library(tidyverse)
library(kableExtra)
library(sf)
library(stars)
library(mapview)
library(randomForest)

knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 40), tidy = TRUE)


```

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css">


.pull-left[
Today we'll ....

- Intro to random forest models


]

.pull-right[


<img src="https://dfzljdn9uc3pi.cloudfront.net/2020/8262/1/fig-1-2x.jpg" width="450">
]

---

# What is a Random Forest?

.pull-left[
- **Ensemble method** built from many decision trees  
- Each tree trained on a **bootstrap sample** of the data  
- Predictions are averaged (regression) or voted (classification)  
- Reduces overfitting and improves generalization  
]

.pull-right[
![](https://miro.medium.com/v2/resize:fit:1184/format:webp/1*i0o8mjFfCn-uD79-F1Cqkw.png)
]

---

# ğŸŒ² What is a Random Forest?

.pull-left[
### Think of it like a team of decision trees!

- Each **tree** makes a prediction (like a vote).  
- Trees are trained on **different random subsets** of the data.  
- Each tree also sees **different random sets of variables**.  
- The forestâ€™s final prediction is the **average (regression)**  
  or **majority vote (classification)** across all trees.

ğŸ’¡ The idea:  
> Many weak but diverse trees together make a strong, stable model.
]

.pull-right[
![](https://miro.medium.com/v2/resize:fit:1184/format:webp/1*i0o8mjFfCn-uD79-F1Cqkw.png)

.small[Each tree sees a different sample of data and features.]
]


---

# ğŸŒ² Classification vs. Regression Forests

.pull-left[
### **Classification Forest**
- Used when the **response variable is a category**  
  (e.g., species, gender, habitat type, house color).
- Each tree â€œvotesâ€ for a class label.
- The **majority vote** across all trees = prediction.

ğŸ“˜ **Example**
```{r}
Species ~ Sepal.Length + Petal.Width
#Predicts setosa, versicolor, or virginica.
```

ğŸ¯ Goal: minimize misclassification error.
]

.pull-right[

### **Regression Forest**
- Used when the response variable is numeric
(e.g., body mass, growth rate, temperature).
- Each tree predicts a number.
- The average across all trees = prediction.

ğŸ“˜ Example

```{r}
Sepal.Width ~ Sepal.Length + Petal.Width
```

ğŸ¯ Goal: minimize mean squared error (MSE).
]


---

# Installing and loading

```{r}
#install.packages("randomForest")
library(randomForest)

#Example dataset:
  
data(iris)
head(iris)
```


---

# âš™ï¸ Fitting a Random Forest in R
## ğŸ”§ Steps
.pull-left[

### 1. Fit model
```{r}
set.seed(123) #for reproducibility
rf_model <- randomForest(
  Species ~ .,      # response ~ predictors
  data = iris,
  ntree = 500,      # number of trees
  mtry = 2, # predictors per split
  importance = TRUE # compute variable importance
)
```
]

.pull-right[

### What these do

- `ntree` â†’ more trees = smoother, more stable forest
- `mtry` â†’ how many predictors are tested at each split
- `importance` â†’ tracks which variables matter most

]

---

# âš™ï¸ Fitting a Random Forest in R

.pull-left[
### 2. Check the model output

```{r}
rf_model

```
]

.pull-right[

### What these do
- check model output

]

---

# âš™ï¸ Fitting a Random Forest in R
.pull-left[
### 2. Check the model output

```{r,fig.height=4}
plot(rf_model)
```
]

.pull-right[

### What these do
- `plot()` â†’ shows error rate vs. number of trees

]

---

# ğŸŒŸ What Variables are Important?

.pull-left[
### ğŸ§  The big idea

- A **Random Forest** learns from many predictors (features).  
- But not all predictors help equally!  
- **Variable importance** shows which ones matter most  
  for making good predictions.

ğŸ’¡ Imagine asking:  
> â€œIf I hide this variable, does the forest get worse at predicting?â€

If accuracy drops a lot â†’ that variable is **important**.  
If accuracy stays the same â†’ that variable probably isnâ€™t helping much.
]

.pull-right[
### ğŸ” Assessing Importances

```{r}
# Show how important each variable is
importance(rf_model)


```


---

# ğŸŒŸ What Variables are Important?

.pull-left[
### ğŸ§  The big idea

- A **Random Forest** learns from many predictors (features).  
- But not all predictors help equally!  
- **Variable importance** shows which ones matter most  
  for making good predictions.

ğŸ’¡ Imagine asking:  
> â€œIf I hide this variable, does the forest get worse at predicting?â€

If accuracy drops a lot â†’ that variable is **important**.  
If accuracy stays the same â†’ that variable probably isnâ€™t helping much.
]

.pull-right[
### ğŸ” Assessing Importances

```{r,fig.height=4}
# Make a quick plot
varImpPlot(rf_model)
```
.small[
  longer bars `=` variables that help the model the most.

shorter bars `=` variables with little impact on predictions.
]
]

---

# ğŸ¯ Making Predictions with a Random Forest

.pull-left[
### ğŸ§  The idea

Once your forest is trained,  
you can use it to **predict new outcomes**!

- The model has â€œlearnedâ€ patterns from training data.  
- You give it new observations (with predictor values).  
- Each tree makes a prediction â†’  
  the forest combines them for a final answer.

ğŸŒ³ Many trees â†’ one decision!
]

.pull-right[
### ğŸ” In R

```{r}
# Make predictions
new_obs <- iris[1:5, -5]    # drop Species column
predict(rf_model, new_obs)
```
ğŸ§© For classification:

Each tree votes for a class. The forest picks the majority vote.

ğŸ“ˆ For regression:

Each tree predicts a number. The forest takes the average.


]

---

# ğŸ§© Comparing Predictions to Actual Classes

.pull-left[
### ğŸ¯ Why compare?
- We want to see **how often the model gets it right!**  
- Comparing predicted and actual classes tells us  
  how accurate our Random Forest is.  
- A **confusion matrix** summarizes correct vs. incorrect predictions.

ğŸ’¡ The more predictions on the diagonal â†’  
the better the model!
]

.pull-right[

### Compare predictions to actual
```{r}
pred <- predict(rf_model, iris)
# Confusion matrix
table(Predicted = pred, Actual = iris$Species)
# Model accuracy
mean(pred == iris$Species)

```
.small[

The confusion matrix shows where the model is right or wrong.

Accuracy = proportion of correct predictions.

]
]